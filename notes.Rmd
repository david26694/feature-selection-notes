---
title: "Feature selection notes"
output: html_document
---

http://www.feat.engineering/classes-of-feature-selection-methodologies.html

Feature selection methods:

* Intrinsic: lasso, trees.
* Filter: given an importance measure, delete variables below some importance threshold.
* Wrapper: iterative, like Recursive feature elimination. They overfit the training set and need external validation.


http://www.feat.engineering/feature-selection-simulation.html#fig:selection-roc

Recall that the main random forest tuning parameter,  mtry, 
is the number of randomly selected variables that should be evaluated at each split. 
While this encourages diverse trees (to improve performance), 
it can necessitate the splitting of irrelevant predictors. 
Random forest often splits on the majority of the predictors 
(regardless of their importance) and will vastly over-select predictors. 
For this reason, an additional analysis might be needed to determine which predictors are truly important essential based on that modelâ€™s variable importance scores. Additionally, trees in general poorly estimate the importance of highly correlated predictors. For example, adding a duplicate column that is an exact copy of another predictor will numerically dilute the importance scores generated by these models. For this reason, important predictors can have low rankings when using these models.


The three tree ensembles (bagging, random forest, and boosting) have excellent true positive rates; the truly relevant predictors are almost always retained. However, they also show poor results for irrelevant predictors by selecting many of the noise variables. This is somewhat related to the nature of the simulation where most of the predictors enter the system as smooth, continuous functions. This may be causing the tree-based models to try too hard to get achieve performance by training deep trees or larger ensembles.


http://www.feat.engineering/selection-overfitting.html


An analogous problem can occur when performing feature selection. For many data sets it is possible to find a subset of predictors that has good predictive performance on the training set but has poor performance when used on a test set or other new data set. The solution to this problem is similar to the solution to the problem of overfitting: feature selection needs to be part of the resampling process.


http://www.feat.engineering/recursive-feature-elimination.html


One notable issue with measuring importance in trees is related to multicollinearity. If there are highly correlated predictors in a training set that are useful for predicting the outcome, then which predictor is chosen for partitioning the samples is essentially a random selection. It is common to see that a set of highly redundant and useful predictors are all used in the splits across the ensemble of trees. In this scenario, the predictive performance of the ensemble of trees is unaffected by highly correlated, useful features


Boruta


http://danielhomola.com/2015/05/08/borutapy-an-all-relevant-feature-selection-method/?source=post_page-----bf47e94e2558----------------------

https://pdfs.semanticscholar.org/85a8/b1d9c52f9f795fda7e12376e751526953f38.pdf

paper: Feature Selection with the Boruta Package